{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arnav\\desktop\\vangogh\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision numpy matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "content_img_path = 'Pictures/Pic2.jpg'\n",
    "style_img_path = 'Pictures/Pic1.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=512):\n",
    "    if not os.path.isfile(img_path):\n",
    "        raise FileNotFoundError(f\"Cannot find image file: {img_path}\")\n",
    "        \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "        if image.width > image.height:\n",
    "            size = (max_size, int(image.height * max_size / image.width))\n",
    "        else:\n",
    "            size = (int(image.width * max_size / image.height), max_size)\n",
    "        image = image.resize(size, Image.LANCZOS)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    return img_tensor, image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    image = tensor.cpu().clone().detach().numpy()\n",
    "    image = image.squeeze(0)\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(tensor, filename):\n",
    "    image = tensor_to_image(tensor)\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    Image.fromarray(image).save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IEST Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNorm(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(InstanceNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mean = torch.mean(x, dim=(2, 3), keepdim=True)\n",
    "        x_var = torch.var(x, dim=(2, 3), keepdim=True) + self.epsilon\n",
    "        x_normalized = (x - x_mean) / torch.sqrt(x_var)\n",
    "        return x_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IESTTransferModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IESTTransferModule, self).__init__()\n",
    "        self.norm = InstanceNorm()\n",
    "        \n",
    "    def forward(self, content_feat, style_feat):\n",
    "        c_mean = torch.mean(content_feat, dim=(2, 3), keepdim=True)\n",
    "        c_std = torch.std(content_feat, dim=(2, 3), keepdim=True) + 1e-8\n",
    "        s_mean = torch.mean(style_feat, dim=(2, 3), keepdim=True)\n",
    "        s_std = torch.std(style_feat, dim=(2, 3), keepdim=True) + 1e-8\n",
    "        \n",
    "        c_normalized = (content_feat - c_mean) / c_std\n",
    "        \n",
    "        channels = content_feat.size(1)\n",
    "        batch_size = content_feat.size(0)\n",
    "        \n",
    "        content_flat = content_feat.view(batch_size, channels, -1)\n",
    "        style_flat = style_feat.view(batch_size, channels, -1)\n",
    "        \n",
    "        content_flat_norm = F.normalize(content_flat, dim=2)\n",
    "        style_flat_norm = F.normalize(style_flat, dim=2)\n",
    "        \n",
    "        correlation = torch.bmm(content_flat_norm, style_flat_norm.transpose(1, 2))\n",
    "        correlation = F.softmax(correlation, dim=2)\n",
    "        \n",
    "        enhanced_style = torch.bmm(correlation, style_flat)\n",
    "        enhanced_style = enhanced_style.view_as(content_feat)\n",
    "        \n",
    "        instance_enhanced = c_normalized * s_std + s_mean\n",
    "        channel_enhanced = enhanced_style\n",
    "        \n",
    "        alpha = 0.6\n",
    "        result = alpha * instance_enhanced + (1 - alpha) * channel_enhanced\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEST Model Definition\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
